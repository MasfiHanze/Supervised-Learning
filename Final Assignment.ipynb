{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e845a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import IFrame\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "\"\"\"\n",
    "For every individual there are 6 records and 22 factors measured against it.\n",
    "\"\"\"\n",
    "parkinsons = pd.read_table('parkinsons.data',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c032d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Dataset Content\n",
    "\n",
    "parkinsons.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ba7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Features In Dataset :', parkinsons.shape[1])\n",
    "print('Number of Instances In Dataset : ', parkinsons.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first column of data contains name and recording number, I thought it is a good idea to split the name and recording\n",
    "#number, just in case it might be beneficial for future groupings\n",
    "\n",
    "parkinsons.insert(1, 'Recording',0)\n",
    "#splitting the data with last underscore\n",
    "parkinsons[['name','Recording']] = parkinsons.name.str.rsplit('_', 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying status column to new Status column at 3rd location for better visibility and understanding of the dataset\n",
    "parkinsons.insert(2, 'Status',parkinsons['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the redundant old status column\n",
    "parkinsons = parkinsons.drop(['status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38813cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking For Duplicate Rows In Dataset\n",
    "print('Number of Duplicated Rows :',parkinsons.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final dataframe\n",
    "parkinsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c70db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data descriptions from the parkinsons.names file. You can find a table in next code block with all feature details\n",
    "description = pd.read_table('parkinsons.names', error_bad_lines=False, warn_bad_lines=False)\n",
    "description = description.iloc[34:47].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Details from ncbi website\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5434464/table/tab2/?report=objectonly', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3872d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd4a42",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4cf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic data exploration, checking for NAN, shape and size\n",
    "\n",
    "#Size of data\n",
    "print(f'Size of data : {parkinsons.size}')\n",
    "#Shape of data\n",
    "print(f'Shape of data : {parkinsons.shape}')\n",
    "#Null values\n",
    "print(f'Number of null values: {parkinsons.isnull().sum().sum()}')\n",
    "\n",
    "\n",
    "#Exploring the dataset to check how many pateint records are there with parkinsons\n",
    "healthy = (parkinsons['Status'] == 0).sum()\n",
    "print(f'Number of pateint records without parkinsons: {healthy}')\n",
    "unhealthy = (parkinsons['Status'] == 1).sum()\n",
    "print(f'Number of pateint records with parkinsons: {unhealthy}')\n",
    "\n",
    "# first 3 columns are name, recording and status, we check for features after 3rd column\n",
    "print(f'Number of features: {len(parkinsons.columns[3:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f19f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Using describe to check the spread of values over the dataset.Though the data seems to be highly dispersed over a wide range of\n",
    "numbers. I choose not to normalise the dataset becuase of 2 reasons. First, before taking the measurements for individuals the\n",
    "amplitude of each signal has already been already been digitally normalized in order to suppress the effects of \n",
    "individual difference. Second, not all features are measured over same scale, Amplitude is \n",
    "measured in Hz(hertz), Jitter in percentage and Shimmer in dB(decibel.) and so on. Normalising it will scale it down to a \n",
    "similar scale and as the dataset is itself very small, there might be a situation where after normalisation we can lose \n",
    "some important data.\n",
    "\"\"\"\n",
    "parkinsons.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the distribution on status over dataset, a boxplot is shown.. It clearly shows that we have very less data\n",
    "# for individuals with parkinsons and individuals not with parkinson. 0 implies indivduals not having parkinsons and\n",
    "# Status of 1 implies individuals with parkinsons\n",
    "\n",
    "\n",
    "count = sns.countplot(x=parkinsons['Status'], label= 'Count',  palette=['#2BD957', '#54DEE4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac045eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining columns for heatmap\n",
    "cols = ['Status','MDVP:Fo(Hz)','MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', \n",
    "        'Jitter:DDP','MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ', 'Shimmer:DDA', 'NHR', \n",
    "        'HNR', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2','PPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e1300",
   "metadata": {},
   "source": [
    "## I wanted to check how the data is co-related so first option was to create a pairplot for all the features with respect to Status column of parkinsons dataframe\n",
    "Orange color in the distribution refers to pateinst with status 1 i.e. having Parkinsons and blue with status 0 i.e. Healthy.\n",
    "As, the data is not evenly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figsize\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.pairplot(data=parkinsons[cols],hue = 'Status')\n",
    "plt.savefig('parkinsonspairplot.png',format='png')\n",
    "\n",
    "#For greater visibility the image is stored so that it can be zoomed in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff7d53",
   "metadata": {},
   "source": [
    "# Second option to check the corelation between featrues was to plot a heatmap.\n",
    "\n",
    "Values which are closer to -1 shows that there is no linear corelation between the 2 factors. Whereas, the values which are\n",
    "closer to one shows that the features are directly proportional to the each other (If there is an incrase in value of one\n",
    "feature the other value also increases). In this plot below, the darker the image the greater is the co-relation.\n",
    "\n",
    "We can see 2 dark blue squares in the heatmap below they are because of the fact that they are plotted against the similar\n",
    "measures. The first blue block at left can be seen for values of Jitter whicg are highly co-related between each other.\n",
    "Similarly, the second blue block is for measures of shimmer.\n",
    "\n",
    "A corelation score of greater than 0.6 is what we are looking for in the heatmap below, but as can be seen we have a huge \n",
    "number of relations to describe for this heatmap. I have selected two of them to give a probable description of the relations.\n",
    "\n",
    "- NHR - MDVP Jitter% : (0.91)\n",
    "  NHR is the ratio of Noise to Harmonic Ratio. And Jitter is the modulation of periodicity of voice signals.\n",
    "  A high degree of jitter results in hoarseness and this can explains well that NHR to be higher and ratio of HNR to be lower \n",
    "  which is the exact opposite of NHR. A lower NHR and higher HNR implies greater noise quality i.e with less jitter. Hence,\n",
    "  jitter is directly proportional to NHR and inversely proportional to HNR.\n",
    " \n",
    "  \n",
    "- PPE - spread1 : (0.96)\n",
    "  PPE represents the inefficiency of voice frequency control.There is a high co-relation score between PPE(pitch period entropy)\n",
    "  and spread 1 which is one of the measures of non-linear fundamental frequency variation. I cannot find why and how they are\n",
    "  co-related but few more reads and digging into details might give some insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "#Calculating the corelations for the heatmap\n",
    "c = parkinsons[cols].corr()\n",
    "sns.heatmap(c,annot=True,cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of observations in each class\n",
    "normalp, parkinsonsp = parkinsons['Status'].value_counts()\n",
    "print('Number of normal individual: ', normalp)\n",
    "print('Number of individuals with parkinsons : ', parkinsonsp)\n",
    "print('')\n",
    "print('% of normal individual ', round(normalp / len(parkinsons) * 100, 2), '%')\n",
    "print('% of of individuals with parkinsons', round(parkinsonsp / len(parkinsons) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142d5b7",
   "metadata": {},
   "source": [
    "# Trying different clasifier\n",
    "### Evaluation function for models\n",
    "\n",
    "The code for model evaluation has been used from: https://github.com/fenna/student_BFVM19DATASC3/blob/main/W03_Ensemble_solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74179323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def evaluate(y_test, y_pred, X_test, clf):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "      \n",
    "def plot_learning_curves(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model:pipeline object\n",
    "        X_train, y_train: training data\n",
    "        X_val, y_val: test data\n",
    "    \"\"\"\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(30, len(X_train)): #(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    \n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=1, label=\"training data\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=1, label=\"validation data\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=10)   \n",
    "    plt.xlabel(\"Training set size\", fontsize=10) \n",
    "    plt.ylabel(\"RMSE\", fontsize=10)     \n",
    "    # compare accuracy train versus test to access overfit \n",
    "    print(f'test  acc: {model.score(X_val, y_val)}')\n",
    "    print(f'train acc: {model.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230fa29",
   "metadata": {},
   "source": [
    "# Classifier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887375c4",
   "metadata": {},
   "source": [
    "In this notebook I have tried almost all classification models avaialble in sklearn library. I wanted to explore which classifier works best and to play around with the data.\n",
    "\n",
    "First the data has been split into train, test and validation sets. No normalisation has been done. Justification has already been given. The validation and test scores have been summed up in a summary table at the end of this notebook. However, for each of the scores the plots should always be considered to come to a conclusion as to which method performs best.\n",
    "\n",
    "\n",
    "1. Logistic regression\n",
    "2. Decision tree\n",
    "3. SVM\n",
    "4. Naive Bayes\n",
    "\n",
    "\n",
    "#### Ensemble Learning\n",
    "1. Random forest\n",
    "2. Bagging with Decicion Tree classifier\n",
    "3. Bagging with KNeighborsClassifier classifier\n",
    "4. Boosting\n",
    "5. Stacking\n",
    "6. Gradient Boosting\n",
    "7. Voting classifier\n",
    "\n",
    "#### Neural network model\n",
    "1. MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d632d94",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e30b7",
   "metadata": {},
   "source": [
    "#### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting features after 3rd column\n",
    "Xs = parkinsons.columns[3:]\n",
    "ys = parkinsons.columns[2:3]\n",
    "\n",
    "# set X and y with the above defined columns\n",
    "X = parkinsons[Xs]\n",
    "y = parkinsons[ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba47a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shaped of X and Y if they are same\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5124156",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the train_test_split function to split the data into train,test and validation. As my dataset\n",
    "# is too small. I have decided to keep the test size to 25% and then 10% validation dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# also create a validation set from the train set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=0)\n",
    "print(f'X train shape: {X_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}')\n",
    "print(f'X validation shape: {X_val.shape}\\n')\n",
    "print(f'y train shape: {y_train.shape}')\n",
    "print(f'y test shape: {y_test.shape}')\n",
    "print(f'y validation shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4441fe",
   "metadata": {},
   "source": [
    "#### Check the distribution for all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4feecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking distribution of train, test and valdiation data\n",
    "from collections import Counter\n",
    "plt.bar(Counter(y_train).keys(), Counter(y_train).values(), color='#2BE957')\n",
    "plt.title('Train')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(Counter(y_test).keys(), Counter(y_test).values(), color='#54DEE8')\n",
    "plt.title('Test')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(Counter(y_val).keys(), Counter(y_val).values())\n",
    "plt.title('Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be62ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting boxplots to check the distribution of Status over all features\n",
    "for column in parkinsons.columns[3:]:   \n",
    "    sns.set()    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.boxplot(x = 'Status', y = column , data = parkinsons )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5864e",
   "metadata": {},
   "source": [
    "#### Defining a summary table to store Training and Testing accuracies of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(columns = ['Name','Training accuracy', 'Testing accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ba4af",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1034c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "lg = LogisticRegression(random_state=0) #max_iter=10000\n",
    "\n",
    "# fit the model on the train data\n",
    "lg.fit(X_train, y_train.values.ravel()) #np.ravel(y_train)\n",
    "\n",
    "# calculate accuracy score on train data\n",
    "lg_score_train = lg.score(X_train, y_train)\n",
    "print(f'Accuracy of logistic regression on train set {lg_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "lg_score_test = lg.score(X_test, y_test)\n",
    "print(f'Accuracy of logistic regression on test set {lg_score_test:.2f}')\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "\n",
    "# predict the values and evaluate with precision, recall and f1 \n",
    "y_pred = lg.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, lg)\n",
    "\n",
    "#Appending scores in summary table\n",
    "lgsummary = {'Name':'Logistic Regression','Training accuracy':lg_score_train,'Testing accuracy':lg_score_test}\n",
    "summary = summary.append(lgsummary, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion matrix for Logistic Regression', y=1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762b53c",
   "metadata": {},
   "source": [
    "Logistic Regression is used to examine the association of (categorical or continuous) independent variable(s) with one dependent variable. It is used for binary classification problem which has only two classes to predict. In our dataset, the 2 classes are having parkinsons or not. I suspected that this model should work but it is not the perfect model to be used.\n",
    "\n",
    "Results:\n",
    "Logistic regression model has a precision of 0.93 \n",
    "Accuracy of 0.94\n",
    "Recall: 1\n",
    "f1-score: 0.96 \n",
    "\n",
    "It has a recall value of 1 which will be perfect in an ideal scenario where every result retrieved by the model is correct.\n",
    "But in my opinion a score of perfect 1 is not a good thing to acheive. It might be possible if we have a larger dataset, then \n",
    "our training and test data might acheieve different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d865b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = lg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed3eff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting the learning curve\n",
    "plot_learning_curves(lg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcdfe2",
   "metadata": {},
   "source": [
    "An RMSE score between 0.2 and 0.5 shows that the model can predict the data correctly. In our case both validation and traning\n",
    "data does seem to acheive a good RMSE score but there is a lot of overfitting.\n",
    "As can be seen in the plot the model starts to train well after about a size of 5. After around 25-40 the model does start to\n",
    "converge but after 70 it starts to overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Logistic Regression Model\n",
    "# joblib.dump(lg, 'lg_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d058b",
   "metadata": {},
   "source": [
    "# 2.Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b05aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "dt = DecisionTreeClassifier()\n",
    "# fit the model on the train data\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aad5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "dt_score_train = dt.score(X_train, y_train)\n",
    "print(f'Accuracy of DecisionTree on train set {dt_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "dt_score_test = dt.score(X_test, y_test)\n",
    "print(f'Accuracy of DecisionTree on test set {dt_score_test:.2f}')\n",
    "print(\"Confusion Matrix:\")\n",
    "y_pred = dt.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, dt)\n",
    "#Appending scores in summary table\n",
    "dtsummary = {'Name':'Decision Tree','Training accuracy':dt_score_train,'Testing accuracy':dt_score_test}\n",
    "summary = summary.append(dtsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197663f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion matrix for Decision Tree', y=1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6325b83",
   "metadata": {},
   "source": [
    "Decision trees are used for handling non-linear data sets effectively.Decision trees can be divided into two types;\n",
    "categorical variable and continuous variable decision trees. It might be a good model to predict house prices based on certain\n",
    "defined conditions or in civil engineering aspects. But in our case decision tree might not be a good idea as we are \n",
    "talking about disease prediction. The major drawback for decision trees is that if there is a slight change in the features of\n",
    "data the results can change abruptly, and considering that it will be a really bad idea to predict any kind of disease/ diagnosis\n",
    "based on decision tree. Decision trees are best suited for models where interpretation of data is lesss important than achieving\n",
    "accuracy.\n",
    "\n",
    "Our model achieves an accuracy of 0.9, precision of 0.97, a recall of 0.89 and f1-score of 0.93 which are all good in terms of \n",
    "statistical terms but as mentioned above a slight change and the model can converge on a negative side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = dt.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the learning curve\n",
    "plot_learning_curves(dt, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ca11a",
   "metadata": {},
   "source": [
    "As can be seen in above learning curve there is a huge difference between training and validation data. It starts to overfit\n",
    "from the beginning and the RMSE score on training data is zero which is a bad indication that this model is not good for \n",
    "predicting parkinson's disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an interesting visualization of the decision tree where the algorithm can draw a tree based on the algorithm\n",
    "# I thought of giving it a try\n",
    "from sklearn import tree\n",
    "plt.figure(figsize=(20, 20))\n",
    "tree.plot_tree(dt,filled=True,\n",
    "              feature_names=parkinsons.columns[2:],\n",
    "              class_names=['healthy', 'parkinsons'],\n",
    "              fontsize=8)  \n",
    "plt.savefig('parkinsons.png',format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea583ff",
   "metadata": {},
   "source": [
    "In the decision tree above it can be clearly seen that spread2 is once defined in class healthy and once in parkinsons which\n",
    "in my opinion is wrong and can raise confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Decision Tree Classifier\n",
    "#joblib.dump(dt, 'dt_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717868c",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f370a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "svm = SVC(kernel='rbf')\n",
    "# fit the model on the train data\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950ee78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "svm_score_train = svm.score(X_train, y_train)\n",
    "print(f'Accuracy of SVM on train set {svm_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "svm_score_test = svm.score(X_test, y_test)\n",
    "print(f'Accuracy of SVM on test set {svm_score_test:.3f}')\n",
    "print(\"confusion Matrix:\")\n",
    "y_pred = svm.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, svm)\n",
    "svmsummary = {'Name':'SVM','Training accuracy':svm_score_train,'Testing accuracy':svm_score_test}\n",
    "summary = summary.append(svmsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion matrix for SVM', y=1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(svm, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc533b7",
   "metadata": {},
   "source": [
    "Support Vector machines should be used when we have defined bouadaries between features. It does not perform well when the dataset\n",
    "features are overlapping as in our case. Jitter, aplitude and shimmer are overlapping features and co-related. It works best \n",
    "for multiclass classification problems.\n",
    "\n",
    "This model has a recall of 1.0 which is again not good and accuracy lower than Linear regression and decision tree model.\n",
    "For our dataset this is not a good choice.\n",
    "\n",
    "The larning curve also shows overfitting from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping SVM Classifier\n",
    "#joblib.dump(svm, 'svm_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3414f",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "gnb = GaussianNB()\n",
    "# fit the model on the train data\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "gnb_score_train = gnb.score(X_train, y_train)\n",
    "print(f'Accuracy of Naive Bayes on train set {gnb_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "gnb_score_test = gnb.score(X_test, y_test)\n",
    "print(f'Accuracy of Naive Bayes on test set {gnb_score_test:.3f}')\n",
    "print(\"confusion Matrix:\")\n",
    "y_pred = gnb.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, gnb)\n",
    "gnbsummary = {'Name':'Naive Bayes','Training accuracy':gnb_score_train,'Testing accuracy':gnb_score_test}\n",
    "summary = summary.append(gnbsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(gnb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1344797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion matrix for Decision Tree', y=1.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = gnb.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217a729",
   "metadata": {},
   "source": [
    "Naive Bayes works best with text classification. It is a probability calssifier and predics based on th epredictibilty\n",
    "of an object. Initially the model starts to learn well but then it starts overfitting after just 6-7 training data. But it does\n",
    "start to converge after sbout 60, so it might be possible that it will work on a dataset greater in size.\n",
    "\n",
    "On the other hand it has a precision of 1.0 which is not good, and when we  see the accuracy it is 0.63 which is the worst \n",
    "performing model up until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c13939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Naive Bayes Classifier\n",
    "#joblib.dump(gnb, 'nb_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bae4de",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d8177",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "rf = RandomForestClassifier(n_estimators = 10)\n",
    "#fit the model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "rf_score_train = rf.score(X_train, y_train)\n",
    "print(f'Accuracy of Random Forest on train set {rf_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "rf_score_test = rf.score(X_test, y_test)\n",
    "print(f'Accuracy of Random Forest on test set {rf_score_test:.3f}')\n",
    "print(\"confusion Matrix:\")\n",
    "y_pred = rf.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, rf)\n",
    "\n",
    "rfsummary = {'Name':'Random Forest','Training accuracy':rf_score_train,'Testing accuracy':rf_score_test}\n",
    "summary = summary.append(rfsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2335cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199aa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = rf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1dfa2",
   "metadata": {},
   "source": [
    "Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through\n",
    "cross validation. But one most important detail is Random Forest is a data hungry algorith so using it in a small dataset\n",
    "is a wrong choice. It creates multiple decision tree and based on that provides a result. As I have already explained\n",
    "why decision tree is a bad idea, same goes for random forest.\n",
    "\n",
    "This model has a precision of 1.0 which means it has got a lot of false negative. The accuracy of 0.93 is good but as can be\n",
    "seen from the plot there is a lot of overfitting and it is not converging at all. So, for parkinsons prediction it is not at\n",
    "all a good choice of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6e7df",
   "metadata": {},
   "source": [
    "# Bagging with Decicion Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def965fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(), max_features = 1.0, max_samples = 0.5) \n",
    "#fit the model\n",
    "bg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "bg_score_train = bg.score(X_train, y_train)\n",
    "print(f'Accuracy of Bagging with DecisionTree on train set {bg_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "bg_score_test = bg.score(X_test, y_test)\n",
    "print(f'Accuracy of Bagging with DecisionTree on test set {bg_score_test:.3f}')\n",
    "print(\"confusion Matrix:\")\n",
    "\n",
    "y_pred = bg.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, bg)\n",
    "bgsummary = {'Name':'Bagging with Decicion Tree','Training accuracy':bg_score_train,'Testing accuracy':bg_score_test}\n",
    "summary = summary.append(bgsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a699a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(bg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9cb0c2",
   "metadata": {},
   "source": [
    "Bagging is a way to decrease the variance in the prediction by generating additional data for training from \n",
    "dataset using combinations with repetitions to produce multi-sets of the original data. In bagging we are using all features\n",
    "in comparison with random forest where we are using a subset of features.\n",
    "\n",
    "This model has an accuracy of 0.88 and a precision of 0.92, recall of 0.92 and an f1-score of 0.92 which is a good score overall.\n",
    "The learning curve also shows better results where training data and validation data might converge but there is a lot \n",
    "of overfitting. But after around 10 records it might perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5eeb5",
   "metadata": {},
   "source": [
    "# Bagging with KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                             max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef40d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "bagging.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "bagging_score_train = bagging.score(X_train, y_train)\n",
    "print(f'Accuracy of Bagging with KNeighbors Classifier on train set {bagging_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "bagging_score_test = bagging.score(X_test, y_test)\n",
    "print(f'Accuracy of Bagging with KNeighbors Classifier on test set {bagging_score_test:.3f}')\n",
    "print('')\n",
    "y_pred = bagging.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, bagging)\n",
    "\n",
    "bgksummary = {'Name':'Bagging with K KNeighbors','Training accuracy':bagging_score_train,'Testing accuracy':bagging_score_test}\n",
    "summary = summary.append(bgksummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535de9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(bagging, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a2fd9",
   "metadata": {},
   "source": [
    "Bagging with knn has better accuracy as compared to previous model i.e. 0.93 and a precision of 0.93. But yet again a recall of\n",
    "1.0 which is not good. But in terms of the learning curve we can see overlap which is good and if we use a larger dataset\n",
    "or use a smoothing algorith for the plots it might give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d7dd1",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the model\n",
    "adb = AdaBoostClassifier(LogisticRegression(), n_estimators = 22,random_state=0, learning_rate = 1)\n",
    "#fit the model\n",
    "adb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc53462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "adb_score_train = adb.score(X_train, y_train)\n",
    "print(f'Accuracy of Ada Boosting on train set {adb_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "adb_score_test = adb.score(X_test, y_test)\n",
    "print(f'Accuracy of Ada Boosting on test set {adb_score_test:.2f}')\n",
    "print('')\n",
    "y_pred = adb.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, adb)\n",
    "plot_learning_curves(adb, X_train, y_train, X_test, y_test)\n",
    "adbsummary = {'Name':'Ada Boosting','Training accuracy':adb_score_train,'Testing accuracy':adb_score_test}\n",
    "summary = summary.append(adbsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa23397",
   "metadata": {},
   "source": [
    "For our dataset even if the model has reached an accuracy of 0.92 it does not perform well if we see the learning curve. \n",
    "After around 5 it starts to overfit. I have used a Logistic Regression classifier for boosting. It might be possible that \n",
    "if any other classifier is used it might give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e692571",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the model\n",
    "estimators = [('dt', dt), ('lg',lg), ('svm', svm), ('gnb', gnb)]\n",
    "#fit the model\n",
    "sclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "sclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb471617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "sclf_score_train = sclf.score(X_train, y_train)\n",
    "print(f'Accuracy of Stacking on train set {sclf_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "sclf_score_test = sclf.score(X_test, y_test)\n",
    "print(f'Accuracy of Stacking on test set {sclf_score_test:.3f}')\n",
    "print('')\n",
    "y_pred = sclf.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, sclf)\n",
    "plot_learning_curves(sclf, X_train, y_train, X_test, y_test)\n",
    "sclfsummary = {'Name':'Stacking','Training accuracy':sclf_score_train,'Testing accuracy':sclf_score_test}\n",
    "summary = summary.append(sclfsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1ac67",
   "metadata": {},
   "source": [
    "Stacking refers to a method of joining the machine learning models, similar to arranging a stack of plates.\n",
    "Output of many models are combined and it is implemented to create a model. It is said that th eperformance is sometimes\n",
    "best as compared to individual models. In our case we have an accuracy of 0.89 with a precision of 0.97, recall of 0.89 and \n",
    "f1-score of 0.93. All these scores point to a good model.\n",
    "\n",
    "The learning curve plot also seem to converge and less overfit as compared to other models. After about 60 the training data\n",
    "starts to overfit.\n",
    "\n",
    "It does take a little bit longer time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6814542",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the model\n",
    "grad = GradientBoostingClassifier(random_state=0)\n",
    "#fit the model\n",
    "grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "grad_score_train = grad.score(X_train, y_train)\n",
    "print(f'Accuracy of Gradient Boosting on train set {grad_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "grad_score_test = grad.score(X_test, y_test)\n",
    "print(f'Accuracy of Gradient Boosting on test set {grad_score_test:.2f}')\n",
    "print('')\n",
    "y_pred = grad.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, grad)\n",
    "plot_learning_curves(grad, X_train, y_train, X_test, y_test)\n",
    "gradsummary = {'Name':'Gradient Boosting','Training accuracy':grad_score_train,'Testing accuracy':grad_score_test}\n",
    "summary = summary.append(gradsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a73b51",
   "metadata": {},
   "source": [
    "Gradient Boosting works well along with Decision trees. Although the model has an accuracy of 0.94 it has a precision of 1.0 \n",
    "which means it has lot of false positives. Also, as mentioned earlier as Decision tree did not work well I assumed that gradient\n",
    "boosting will give certainly bad results which is evident from the learning curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013ecae",
   "metadata": {},
   "source": [
    "# Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696112e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize the model\n",
    "evc = VotingClassifier(estimators = [('dt', dt), ('lg',lg), ('svm', svm)], voting = 'hard')\n",
    "#fit the model\n",
    "evc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "evc_score_train = evc.score(X_train, y_train)\n",
    "print(f'Accuracy of Voting classifier on train set {evc_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "evc_score_test = evc.score(X_test, y_test)\n",
    "print(f'Accuracy of Voting classifier on test set {evc_score_test:.2f}')\n",
    "print('')\n",
    "y_pred = evc.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, evc)\n",
    "plot_learning_curves(evc, X_train, y_train, X_test, y_test)\n",
    "evcsummary = {'Name':'Voting Classifier','Training accuracy':evc_score_train,'Testing accuracy':evc_score_test}\n",
    "summary = summary.append(evcsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51532c43",
   "metadata": {},
   "source": [
    " Voting classifier trains on an ensemble of multiple models and then predicts the values based on the highest votes. Our model has an accuracy of 0.94 with precision of 0.93 which is good but again a recall of 1. As can be seen it starts to overfit form the beginning and might not be a good idea to use for the parkinsons dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316f36d",
   "metadata": {},
   "source": [
    "# MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ef1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the model\n",
    "mlp =  MLPClassifier(alpha=1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac95d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "mlp_score_train = mlp.score(X_train, y_train)\n",
    "print(f'Accuracy of MLP classifier on train set {mlp_score_train:.2f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "mlp_score_test = mlp.score(X_test, y_test)\n",
    "print(f'Accuracy of MLP classifier  on test set {mlp_score_test:.2f}')\n",
    "print('')\n",
    "y_pred = mlp.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, mlp)\n",
    "plot_learning_curves(mlp, X_train, y_train, X_test, y_test)\n",
    "mlpsummary = {'Name':'MLP Classifier','Training accuracy':mlp_score_train,'Testing accuracy':mlp_score_test}\n",
    "summary = summary.append(mlpsummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f43d42",
   "metadata": {},
   "source": [
    "MLP classifier solves problem stochiatically. It is suitable for regression prediction problems where a value needs to be\n",
    "predicted. I assumed that it will not work well and the plot shows why. It has an accuracy of 0.88 with a preciison of 0.92\n",
    ", on the other hand the model is highly overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9e9d5",
   "metadata": {},
   "source": [
    "# QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize the model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "#fit the model\n",
    "qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa40c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score on train data\n",
    "qda_score_train = qda.score(X_train, y_train)\n",
    "print(f'Accuracy of QuadraticDiscriminantAnalysis on train set {qda_score_train:.3f}')\n",
    "\n",
    "# calculate accuracy score on test data\n",
    "qda_score_test = qda.score(X_test, y_test)\n",
    "print(f'Accuracy of QuadraticDiscriminantAnalysis on test set {qda_score_test:.3f}')\n",
    "print('')\n",
    "y_pred = qda.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test, qda)\n",
    "plot_learning_curves(qda, X_train, y_train, X_test, y_test)\n",
    "qdasummary = {'Name':'QuadraticDiscriminantAnalysis','Training accuracy':qda_score_train,'Testing accuracy':qda_score_test}\n",
    "summary = summary.append(qdasummary, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0dc1d",
   "metadata": {},
   "source": [
    "I tried this method just to check how it performs. The resources says it is good to find a non-linear boundary between classifiers\n",
    "The parkinsons dataset does not have the same so I assume it is not a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44587c43",
   "metadata": {},
   "source": [
    "# Summary of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ae9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e3c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
